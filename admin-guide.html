<html>
<head>
<title>Vac site admin guide</title>
</head>
<body>

<h1 align=center>Vac Site Admin Guide<!-- version --></h1>

<p align=center><b>Andrew McNab &lt;Andrew.McNab&nbsp;AT&nbsp;cern.ch&gt;</b>

<h2 style="border-bottom: 1px solid">Quick start</h2>

<p>
By following this quick start recipe you can verify that your installation 
will work with Vac and see it creating and destroying virtual machines. You
will almost certainly want to start again from scratch by following the
step-by-step part of the Admin Guide so don't invest a lot of time here.
If you're already familiar with VMs, you could just skip straight there
but it's safest to go through the quick start to make sure the requirements
are all there.

<p>
To follow the quick start, you need an x86_64 Intel or AMD machine 
with hardware virtualization (Intel VT-x or AMD-V) enabled in its BIOS; and
the machine needs to be installed with a version of Scientific Linux 6, 
with libvirt installed and enabled. In particular, the packages
libvirt, libvirt-client, libvirt-python, qemu-kvm, and then
run &quot;service libvirtd restart&quot; to make sure libvirtd daemon is
running. 

<p>
Install the vac RPM and at the command line excecute:
<pre>
virsh list
virsh create /var/lib/vac/doc/testkvm.xml
virsh list
virsh destroy testkvm
virsh list
</pre>
You should see no VMs listed as running to start with. After the create 
command, the testkvm VM should be listed as running. Afer destroying it,
an empty list of VMs should be returned. If all this doesn't happen, 
then something is wrong with your installation or hardware virtualization 
isn't enabled. Please check the libvirt documentation to try to identify
where the problem is.

<p>
To start using Vac to manage VMs, it's necessary to have a standard NFS
server installed and running. Vac uses NFS to share some directories from
the factory machine to its virtual machines.
It's not necessary to configure the NFS server, as Vac uses exportfs
commands to create and destroy exports dynamically. If you have any iptables
rules blocking NFS you should disable them before starting vacd.
The factory machine must have a fully qualified domain name (FDQN) as
its hostname. So factory1.example.com not just factory1. 
The 169.254.0.0 network should not be configured on the factory machine
before you start Vac. In particular, Zeroconf support should be disabled
by adding NOZEROCONF=yes to /etc/sysconfig/network and restarting
networking.

<p>
Next create the /etc/vac.conf configuration file. Copy
/var/lib/vac/doc/example.vac.conf to /etc/vac.conf and read through its
comments. There are 4 lines you need to check and probably change.

<dl>
<dt><b>vac_space =</b> in [settings]
<dd>Set this to vac01 in your site's domain. So if your site is .example.com
then set it to vac01.example.com . A Vac space is a group of factory
machines that communicate with each other, and is equivalent to a subcluster
or subsite. A space's name is a fully qualified domain name (FQDN), and can be 
used as a virtual CE name where necessary in other systems.

<dt><b>factories =</b> in [settings]
<dd>Since we're creating a space that contains a single factory machine, 
    set this to be the FQDN of the factory machine you're workng on.

<dt><b>total_machines</b>
<dd>Set this to the number of VMs to create and manage on this factory.
    Vac will create hostnames for the VMs from the factory name. For
    example, factory1.example.com will lead to factory1-00.example.com,
    factory1-01.example.com, ...
  
<dt><b>rootpublickey =</b> in [vmtype example]
<dd>This setting is not strictly necessary but is very useful. By copying
    an RSA key pair to /root/.ssh on the factory machine, or creating
    one with ssh-keygen you will be able to ssh into the VM as root and
    see how it is laid out and how it is running. If you don't
    place a public key at the location given in this setting, you need 
    to comment out this setting.
</dl>

<p>
The files needed for the example vmtype are fetched over HTTPS, as 
indicated by the root_disk and user_data options which should not be
changed. 

<p> 
Just do <b>service vacd restart</b>
to make sure vacd is running and look in the log files.

<p>
When vacd starts it forks a factory process that watches the VMs and
creates or destroys them as necessary; and a responder process that
replies to queries from factories about what is running on this host.
These two processes have separate log files as /var/log/vacd-factory
and /var/log/vacd-responder . 

<p>
In its log file, you should be able to see the factory
daemon trying to decide what to do and then creating the example
VM which runs for 5 minutes then shuts itself down. When deciding
what to do, the factory queries its own responder via UDP and this
should be visible in the responder's log file.

<p>
You should also be able to see the state of the VM using the
command <b>vac scan</b>, where vac is a command line tool that the
RPM installs in /usr/sbin.

<h2 style="border-bottom: 1px solid">Configuration step-by-step</h2>

<p>
This part of the guide covers the same ground as the quick start
guide but in a lot more detail. It's intended to help you choose
how best to configure your site.

<p>
The configuration file /etc/vac.conf uses the Python ConfigParser syntax, 
which is similar to MS
Windows INI files. The file is divided into sections, with each section
name in square brackets. For example: [settings]. Each section contains
a series of option=value pairs. Sections with the same name are merged
and if options are duplicated, later values overwrite values given
earlier. 
Any configuration file ending in .conf in the
directory /etc/vac.d will also be read. These files are read in 
alphanumeric order, and then /etc/vac.conf is read if present.

<p>
Based on this ordering in /etc/vac.d/, options from space.conf 
would override any given
in site.conf, but themselves be overwritten by options from 
subspace.conf or vacfactory.conf .

<p>
One
useful approach is to populate /etc/vac.d with a management system
like Puppet, and only create /etc/vac.conf manually to override the
state on individual development machines or if a machine is being
drained of work for maintenance. Site-wide configuration, such 
as vmtype definitions, can be included in /etc/vac.d files present on 
every factory, but host specific options, such as HEPSPEC06 values 
and the total number of VMs to create, can be given in /etc/vac.d
files which are specific to particular subsets of machines. Vac 
will merge all of this information as outlined above.

<h3>Xen vs kvm</h3>

<p>
We recommend that hardware virtualization (eg Intel VT-x features)
with kvm is used for production. Vac also supports Xen 
paravirtualization which can run on old machines without hardware
virtualization but this is not supported on RHEL6/SL6. (6.x RPMs
are <a href="http://xen.crc.id.au/support/guides/install/">available
from Steven Haigh</a>.)

<h3>CernVM images</h3>

<p>
Vac currently requires the use of CernVM images with HEPiX 
contexualization based on EC2/ISO (&quot;CD-ROM&quot;) images,
and we recommend the use of CernVM 3 micro boot images.

<p>
If you need to download an image, they can be found on   
the <a href="http://cernvm.cern.ch/portal/downloads">CernVM 
downloads page</a>. <b>You must get the 
generic .iso image file and not the .hdd file listed for KVM.</b> 

<p>
However, most experiments will supply you with their own
URL from which Vac can automatically fetch their current
designated image version, which Vac caches in /var/lib/vac/imagecache .

<h3>DNS, IP, MAC</h3>

<p>
The factory machines must have fully qualified domain names (FDQN) as
their hostname. So factory1.example.com etc, not just factory1. 

<p>
Vac uses a private NAT network for the set of virtual machines on a
given factory. Vac then creates the VM FQDNs from the factory name
by adding -00, -01, ... So factory1.example.com has factory1-00.example.com,
factory1-01.example.com, ... as its VMs. The total number of virtual
machines on the factory is specified by total_machines. Vac assigns IP
addresses starting 169.254.169.0 for VM 0, 169.254.169.1 for VM 1 etc. 
Unique MAC addresses are also assigned to each VM. 
Using libvirt NAT machinery means this network is
hidden from the rest of the LAN and only visible from the factory and its
VMs. libvirt configures the dnsmasq server to run dedicated DNS and DHCP
servers on this private network. The factory's address in this private
network is 169.254.169.254, which is the so-called Magic IP used by some
Cloud systems for a local configuration service which VMs look for.

<p>
To use IP addresses in the 169.254.0.0 network, <b>you must ensure you are
using a recent
version of dnsmasq.</b> For SL6, dnsmasq-2.48-13.el6.x86_64.rpm avaiable as part
of SL6 updates, is suitable.

<p>
The 169.254.0.0 network should not be configured on the factory machine
before you start Vac. For example, Zeroconf support can be disabled
by adding NOZEROCONF=yes to /etc/sysconfig/network and restarting
networking.

<p>
Vac tries to create the vac private network itself if it doesn't already
exist. You can check the private network exists with the command 
virsh net-list --all which should list the vac_169.254.0.0 network and
the &quot;default&quot; network defined by libvirtd as both being active. 
The vac network should be using the virbr1 virtual interface, with virbr0 
still used by the default network. The vac network is set up as persistent
and auto-starting, so it should survive reboots and restarts of libvirtd.

<p>
However, Vac will log the error &quot;Failed to create NAT network
vac_169.254.0.0&quot; if it doesn't exist and cannot be created. This is 
usually after a restart or an upgrade of libvirtd which leaves settings
which block the new network. Vac will attempt to fix this too unless the
setting fix_networking = false is given in the Vac configuration.

<p>
Network problems are usually caused by: 
<ul>
<li>Still needing to upgrade dnsmasq to an RPM >= 2.48-13
<li>Not disabling Zeroconf
<li>An old dnsmasq process running with argument &quot;--listen-address 
    169.254.169.254&quot; (Use &quot;service dnsmasq stop&quot;)
<li>virbr1 already existing (remove with &quot;ifconfig virbr1 down&quot; and
    &quot;brctl delbr virbr1&quot;, using brctl from the RPM bridge-utils.)
</ul>

<p>
libvirtd attempts to configure the factory machine's iptables rules to
support network address translation / masquerading from the private NAT
network. In addition, you need to allow NFS mounts of directories exported
by the factory to the VM. Assumming the virbr1 virtual interface is used
for the private network, then iptables can be set up using the
iptables-restore command without relying on libvirtd:

<pre>
# Set up masquerading from private network for the VMs
*nat
:PREROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -p tcp -j MASQUERADE ---to-ports 1024-65535 
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -p udp -j MASQUERADE ---to-ports 1024-65535 
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -j MASQUERADE 
COMMIT
# Filtering and forwarding rules
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
# Catch-all, including NFS
-A INPUT -i virbr1 -p udp -j ACCEPT 
-A INPUT -i virbr1 -p tcp -j ACCEPT 
# Forward to/from private network
-A FORWARD -d 169.254.0.0/16 -o virbr1 -m state --state RELATED,ESTABLISHED --j ACCEPT 
-A FORWARD -s 169.254.0.0/16 -i virbr1 -j ACCEPT 
-A FORWARD -i virbr1 -o virbr1 -j ACCEPT 
-A FORWARD -o virbr1 -j REJECT --reject-with icmp-port-unreachable 
-A FORWARD -i virbr1 -j REJECT --reject-with icmp-port-unreachable 
COMMIT
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
# Create checksums for DHCP clients even when using virtio
-A POSTROUTING -o virbr1 -p udp -m udp --dport 68 -j CHECKSUM --checksum-fill 
COMMIT
</pre>

This approach allows you to integrate the extra rules needed for the VMs
private network into any existing rules you use at your site.

<p>
Vac performs a quick check of the current iptables rules at the start of each
cycle using the iptables-save command. If any obvious problems are identified, 
then &quot;Failed to match XXX in output of iptables-save.
Have the NAT rules been removed?&quot; lines will be included in
/var/log/vacd-factory . However, if you have an unusual set of rules, these 
checks may produce false warnings, and they are not an exhaustive check that
the rules you have are sufficient for Vac.

<h3>Logical volumes</h3>

<p>
Vac virtual machines can use logical volumes on the factory machine
to provide additional, faster disk space. Normally these will be mounted at
/scratch by the VM, as they are on conventional grid worker nodes. 

<p>By
default, the block device associated with the logical volume is available to
the VM is vdb, but this can changed with the scratch_device option in a
[vmtype ...] section. 

<p>
The global volume_group option in [settings] (default
vac_volume_group) and the virtual machine's name are used to construct the
logical volume path for each VM. For example, 
/dev/vac_volume_group/factory1-01.example.com

<p>
You <b>must</b> create the volume group with a name that matches the volume_group
setting and you <b>may</b> create the individual logical volumes, with something 
similar to this, where /dev/sda9 is a physical volume created 
during Kickstart or with pvcreate:
<pre>
vgcreate vac_volume_group /dev/sda9
lvcreate --name factory1-00.example.com -L 40G vac_volume_group
lvcreate --name factory1-01.example.com -L 40G vac_volume_group
lvcreate --name factory1-02.example.com -L 40G vac_volume_group
...
</pre>

<p>
During the creation of each virtual machine instance, Vac will attempt to
create the logical volume in volume_group with lvcreate if it doesn't exist 
already. The setting scratch_gb (default 40) will be used when doing this.
Vac will then check the logical volume is satisfactory,
measure its size using lvs, and provide it to the VM as vdb by default.
 
<h3>Installation of Vac: tar vs RPM</h3>

<p>
RPM is the recommended installation procedure, and RPMs are available
from the <a href="http://www.gridpp.ac.uk/vac/versions/">Downloads
directory</a> on the Vac website. 

<p>
It is also possible to install Vac from a tar file, using the install Makefile
target. 

<h3>Configuration of the Vac space</h3>

<p>
The [settings] section must include a vac_space name, which is also used
as the virtual CE name. 

<p>
The factories option takes 
a space separated list of the fully qualified domain names of all
the factories in this Vac space, including this factory. The factories are
queried using UDP when a factory needs to decide which vmtype to start.
The Vac responder process on the factories replies to these queries with
a summary of the VM and the outcome of recent attempts to run a VM of each
vmtype.

<p>
For ease of management, the factories option can be placed in its own
[settings] section in a separate
configuration file in /etc/vac.d which can be automatically generated 
and maintained from another source, such as the site's assets database.

<h3>Setting up vmtypes</h3>

<p>
One [vmtype ...] section must exist for each vmtype in the system, with
the name of the vmtype given in the section name, such as [vmtype example].
A vmtype name must only consist of lowercase letters, numbers,
and hyphens. The vac.conf(5) man page lists the options
that can be given for each vmtype.

<p>
The target_share option for the vmtype gives
the desired share of the total VMs available in this space for that
vmtype. The shares do not need to add up to 1.0, and if a share is not given
for a vmtype, then it is set to 0. The creation of new VMs can be completely
disabled by setting all shares to 0. Vac factories consult these shares
when deciding which vmtype to start as VMs become available.

<p>
For ease of management, the target_shares options can be grouped 
together in a separate file in /etc/vac.d apart from the main [vmtype ...]
sections, which is convenient if shares
are generated automatically or frequently edited by hand and pushed
out to the factory machines. For example:
<pre>
[vmtype example1]
target_share = 5.0
[vmtype example2]
target_share = 6.0
[vmtype example3]
target_share = 7.0
</pre>

<p>
The experiment or VO responsible for each vmtype should supply 
step by step intructions on how to set up the rest of the [vmtype ...]
section and how to create the files to be placed in its subdirectory
of /var/lib/vac/vmtypes (likely to be a hostcert.pem and hostkey.pem
pair to give to the VM.)

<h2 style="border-bottom: 1px solid">Starting and stopping vacd</h2>

<p>
The Vac daemon, vacd, is started and stopped by /etc/rc.d/init.d/vacd 
on conjunction with the usual service and chkconfig commands. As the 
configuration files are reread at the start of each cycle (by default, 
one per minute) <b>it is not necessary to restart vacd after changing the 
configuration</b>.

<p>
Furthermore, as vacd rereads the current state of the VMs from status
files and the hypervisor at the start of each cycle, vacd can be 
restarted without disrupting running VMs or losing information about
their state. 
In most cases it will even be possible to upgrade vacd from one patch
level to another within the same minor release without having to
drain the factory of running VMs. If problems arise during upgrades,
the most likely outcome is that Vac will fail to create new VMs until 
the configuration is fixed, but the existing VMs will continue to run.
(&quot;We want Vac failures to look like planned draining.&quot;) 
Furthermore, since Vac factory machines are autonomous, it is 
straightforward to upgrade one factory in a production Vac space
to check the consequences.

<h2 style="border-bottom: 1px solid">Using the vac command</h2>

<p>
The vac(1) man page explains how the vac command can be used to
scan the current Vac space and display the VMs running, along with
statistics about their CPU load and wall clock time.

<h2 style="border-bottom: 1px solid">Setting up Nagios</h2>

<p>
The check-vacd script installed in /var/lib/vac/bin can be used with
Nagios to monitor the state of the vacd on a factory node. 

<p>
It can be run from the local Nagios nrpe daemon with a line like this
in its configuration file:

<pre>
command[check-vacd]=/var/lib/vac/bin/check-vacd 600
</pre>

which raises an alarm if the vacd heartbeat wasn't updated in the
last 600 seconds.

<h2 style="border-bottom: 1px solid">APEL accounting</h2>

<p>
When Vac detects that a VM has run for at least fizzle_seconds and
now finished, it writes a copy of the APEL
accounting message to subdirectories of /var/lib/vac/apel-outgoing and
/var/lib/vac/apel-archive . Vac uses the UUID of the VM as the local job 
ID, the factory hostname as the local user ID, and the vmtype name as the
batch queue name. A unique user DN is constructed from the components 
of the Vac space name. For example, vac01.example.com becomes
/DC=com/DC=example/DC=vac01 . If the accounting_fqan option is present in
the [vmtype ...] section, then for VMs of that type the value of that option 
is included as the user FQAN, which indicates the VO associated with the VM.

<p>
These accounting messages are designed to be published to the central
APEL service using the
standard APEL ssmsend command, which can be run on each factory machine
from cron. Please see the <a href="https://wiki.egi.eu/wiki/APEL">APEL 
SSM client documentation for details</a>. One you have agreed use of APEL
with the APEL team, had your certificate authorized, and done any requested
tests, it should be sufficient that: you install the apel-ssm RPM on each 
machine, install a host certificate (vac-apel-cert.pem) and key 
(vac-apel-key.pem) authorized to talk to APEL in /etc/grid-security, and 
arrange to run the ssmsend command from cron. 

<p>
The ssmsend command can safely be run multiple times per day as it does
not connect to APEL if there are no new messages, and deletes messages 
once they are sent. It can be run hourly and made to 
use vac-ssmsend-prod.cfg installed by the Vac RPM, by
placing the file vac-ssmsend-cron in /etc/cron.d:
<pre> 
22 * * * * root /usr/bin/ssmsend -c /etc/apel/vac-ssmsend-prod.cfg >>/var/log/vac-ssmsend-cron.log 2>&amp;1
</pre>

<p>
If you use the vac-ssmsend-prod.cfg file for production, please change the 
value of the bdii option to a local or regional top bdii to avoid loading 
the default service included in the file.

<h2 style="border-bottom: 1px solid">Puppet</h2>

<p>
A simple Puppet module for Vac exists as the file init.pp which is installed
in the /var/lib/vac/doc directory. There are extensive comments at the start
of the file which outline how to use it.

<!-- Backoff tuning -->

</body>
</html>
