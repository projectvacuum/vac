<html>
<head>
<title>Vac site admin guide</title>
</head>
<body>

<h1 align=center>Vac Site Admin Guide<!-- version --></h1>

<p align=center><b>Andrew McNab &lt;Andrew.McNab&nbsp;AT&nbsp;cern.ch&gt;</b>

<h2>Quick start</h2>

<p>
By following this quick start recipe you can verify that your installation 
will work with Vac and see it creating and destroying virtual machines. You
will almost certainly want to start again from scratch by following the
step-by-step part of the Admin Guide so don't invest a lot of time here.
If you're already familiar with VMs, you could just skip straight there
but it's safest to go through the quick start to make sure the requirements
are all there.

<p>
To follow the quick start, you need an x86_64 Intel or AMD machine 
with hardware virtualization (Intel VT-x or AMD-V) enabled in its BIOS; and
the machine needs to be installed with a version of Scientific Linux 6, 
with libvirt installed and enabled. In particular, the packages
libvirt, libvirt-client, libvirt-python, qemu-kvm, and then
run &quot;service libvirtd restart&quot; to make sure libvirtd daemon is
running. 

<p>
Install the vac RPM and copy the file /var/lib/vac/doc/testkvm.xml to /tmp .

<p>
At the command line, excecute
<pre>
virsh list
virsh create testkvm.xml
virsh list
virsh destroy testkvm
virsh list
</pre>
You should no VMs listed as runining to start with. After the create 
command, the testkvm VM should belisted as running. Afer destroying it,
an empty list of VMs should be returned. If all this doesn't happen, 
then something is wrong with yourinstallation or hardware virtualization 
isn't enabled. Please check the libvirt documentation to try to identify
where the problem is.

<p>
To start using Vac to manage VMs, it's necessary to have a standard NFS
server installed and running. Vac uses NFS to share some directories from
the factory machine to its virtual machines. and needs to have the standard 
NFS server installed and running.
It's not necessary to configure the NFS server, as Vac uses exportfs
commands to create and destroy exports dynamically. If you have any iptables
rules blocking NFS you should disable them before starting vacd.





<p>
Now you need to create the /etc/vac.conf configuration file. Copy
/var/lib/vac/doc/example.vac.conf to /etc/vac.conf and read through its
comments. There are 6 lines you need to check and probably change.

<dl>
<dt><b>vac_space =</b> in [settings]
<dd>Set this to vac01 in your site's domain. So if your site is .example.com
then set it to vac01.example.com . A Vac space is a group of factory
machines that communicate with each other, and is equivalent to a subcluster
or subsite. A space's name is a fully qualified domain name (FQDN), and can be 
used as a virtual CE name where necessary in other systems.

<dt><b>names =</b> in [factories]
<dd>Since we're creating a space that contains a single factory machine, 
    set this to be the FQDN of the factory machine you're workng on.

<dt><b>total_machines</b>
<dd>Set this to the number of VMs to create and manage on this factory.
    Vac will create hostnames for the VMs from the factory name. For
    example, factory1.example.com will lead to factory1-00.example.com,
    factory1-01.example.com, ...
    
<dt><b>root_image =</b> in [vmtype example]
<dd>The path and filename given in this setting must point to the CernVM
    image you created for Vac in /var/lib/vac/images. Double check the
    version numbers are correct.
  
<dt><b>rootpublickey =</b> in [vmtype example]
<dd>This setting is not strictly necessary but is very useful. By copying
    an RSA key pair to /root/.ssh on the factory machine, or creating
    one with ssh-keygen you will be able to ssh into the VM as root and
    see how it is laid out and how it is running. If you don't
    place a public key at the location given in this setting, you need 
    to comment out this setting.
</dl>

<p>
The files needed for the example vmtype are installed by the RPM in
/var/lib/vac/vmtypes/example and with /etc/vac.conf done and the CernVM
image in place you're ready to go. Just do <b>service vacd restart</b>
to make sure vacd is running and look in the log files.

<p>
When vacd starts it forks a factory process that watches the VMs and
creates or destroys them as necessary; and a responder process that
replies to queries from factories about what is running on this host.
These two processes have separate log files as /var/log/vacd-factory
and /var/log/vacd-responder . 

<p>
In its log file, you should be able to see the factory
daemon trying to decide what to do and then creating the example
VM which runs for 10 minutes then shuts itself down. When deciding
what to do, the factory queries its own responder via UDP and this
should be visible in the responder's log file.

<p>
You should also be able to see the state of the VM using the
command <b>vac scan</b>, where vac is a command line tool that the
RPM installs in /usr/sbin.

<h2>Configuration step-by-step</h2>

<p>
This part of the guide covers the same ground as the quick start
guide but in a lot more detail. It's intended to help you choose
how best to configure your site.

<p>
The configuration file /etc/vac.conf uses the Python ConfigParser syntax, 
which is similar to MS
Windows INI files. The file is divided into sections, with each section
name in square brackets. For example: [settings]. Each section contains
a series of option=value pairs. Sections with the same name are merged
and if options are duplicated, later values overwrite values given
earlier. 
Any configuration file ending in .conf in the
directory /etc/vac.d will also be read. These files are read in 
alphanumeric order, and then /etc/vac.conf is read if present.

<p>
So in /etc/vac.d/, options from space.conf will be override any given
in site.conf, but themselves be overwritten by options from 
subspace.conf or vacfactory.conf .

<p>
One
useful approach is to populate /etc/vac.d with a management system
like Puppet, and only create /etc/vac.conf manually to override the
state on individual development machines or if a machine is being
drained of work for maintenance. Site-wide configuration, such 
as vmtype definitions, can be included in /etc/vac.d files present on 
every factory, but host specific options, such as HEPSPEC06 values 
and the total number of VMs to create, can be given in files which
are specific to particular subsets of machines. Vac will merge
all of this information as outlined above.

<h3>Xen vs kvm</h3>

<p>
We recommend that hardware virtualization (eg Intel VT-x features)
with kvm is used for production. Vac also supports Xen 
paravirtualization which can run on old machines without hardware
virtualization but this is not supported on RHEL6/SL6. (6.x RPMs
are <a href="http://xen.crc.id.au/support/guides/install/">available
from Steven Haigh</a>.)

<h3>CernVM 2 &amp; 3 images</h3>

<p>
Vac currently requires the use of CernVM images with HEPiX 
contexualization based on EC2/ISO (&quot;CD-ROM&quot;) images.

<p>
You can download a recent compressed kvm-style CernVM 2 batch 
image from the <a href="http://cernvm.cern.ch/portal/downloads">CernVM 
downloads page</a>. You can use gunzip or zcat to extract the image
itself. It will be a large file: more than 9GB. gunzip creates
non-sparse files but you can convert the image to sparse with cp:
<pre>
cd /tmp
zcat cernvm-batch-node-2.6.0-4-1-x86_64.hdd.gz >cernvm-batch-node-2.6.0-4-1-x86_64.hdd
cp --sparse=always cernvm-batch-node-2.6.0-4-1-x86_64.hdd /var/lib/vac/images
</pre>
Sparse files not only use less disk space and are quicker to 
copy, but they are also quicker to read.

<p>
The cernvm-batch-node-2.6.0-4-1-x86_64 image was distributed with a
filesystem check interval of 6 months, and after 3rd April 2013 a check
automatically happened when it was used as a Linux root partition. You
can disable this problem using tune2fs:
<pre>
losetup /dev/loop7 cernvm-batch-node-2.6.0-4-1-x86_64.hdd
kpartx -a /dev/loop7
tune2fs -i 0 -c -1 /dev/mapper/loop7p1
kpartx -d /dev/loop7 
losetup -d /dev/loop7
</pre>
If loop7 is in use on your system, replace it with a different number.
For different partitions layouts, you will need to adapt the use of
kpartx and the partition numbers.

<p>
From 0.15.0, Vac can also use Micro CernVM 3 ISO images. These can be
downloaded from the <a href="http://cernvm.cern.ch/portal/downloads">CernVM
downloads page</a> and are much smaller (~12MB) <b>You must get the 
generic .iso image file and not the .hdd file listed for KVM.</b> 

<p>
Alternatively, you can use the cernvm3iso.spec file from /var/lib/vac/doc
to build a binary RPM. This SPEC file is self-contained, in that it 
downloads the ISO image itself, using the major, minor, and release version 
numbers
given in the first few lines of the SPEC file. When the RPM is installed,
the ISO file will be placed in /usr/share/images. The major and minor
numbers are included in the package name so you can install RPMs containing
more than one version at the same time.
You can build the RPM with something like  rpmbuild -ba cernvm3iso.spec

<h3>DNS, IP, MAC</h3>

<p>
Vac uses a private NAT network for each set of virtual machines on a
given factory. Vac then creates the VM FQDNs from the factory name
by adding -00, -01, ... So factory1.example.com has factory1-00.example.com,
factory1-01.example.com, ... as its VMs. The total number of virtual
machines on the factory is specified by total_machines. Vac assigns IP
addresses starting 169.254.169.0 for VM 0, 169.254.169.1 for VM 1 etc. 
Unique MAC addresses are also assigned to each VM. 
Using libvirt NAT machinery means this network is
hidden from the rest of the LAN and only visible from the factory and its
VMs. libvirt configures the dnsmasq server to run dedicated DNS and DHCP
servers on this private network. The factory's address in this private
network is 169.254.169.254, which is the so-called Magic IP used by some
Cloud systems for a local configuration service which VMs look for.

<p>
To use IP addresses in the 169.254.0.0 network, <b>you must ensure you are
using a recent
version of dnsmasq.</b> For SL6, dnsmasq-2.48-13.el6.x86_64.rpm avaiable as part
of SL6 updates, is suitable.

<p>
The 169.254.0.0 network should not be configured on the factory machine
before you start Vac. In particular, Zeroconf support should be disabled
by adding NOZEROCONF=yes to /etc/sysconfig/network and restarting
networking.

<p>
You can check the vac private network exists with the command 
virsh net-list which should list the vac_169.254.0.0 network and
the &quot;default&quot; network defined by libvirtd. The Vac network should
be using the virbr1 virtual interface, with virbr0 still used by the default
network.

<p>
Vac will print the error &quot;Failed to create NAT network vac_169.254.0.0 
(Need dnsmasq RPM >= 2.48-13? Did you disable Zeroconf? Does
virbr1 already exist?)&quot; if it cannot use libvirtd to create that
network. Check dnsmasq is installed, that Zeroconf is disabled, and that
virbr1 does not already exist with the commands 
ifconfig <b>and</b> brctl show. If virbr1 is already there, use 
ifconfig virbr1
down and brctl delbr virbr1 to remove it completely. brctl is from the RPM
bridge-utils.

<p>
If creation of the NAT network fails with &quot;dnsmasq: failed to bind 
listening socket for 169.254.169.254: Address already in use&quot; then
you may be able to clear the problem with &quot;service dnsmasq stop&quot;.

<p>
libvirtd attempts to configure the factory machine's iptables rules to
support network address translation / masquerading from the private NAT
network. In addition, you need to allow NFS mounts of directories exported
by the factory to the VM. Assumming the virbr1 virtual interface is used
for the private network, then iptables can be set up using the
iptables-restore command without relying on libvirtd:

<pre>
# Set up masquerading from private network for the VMs
*nat
:PREROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -p tcp -j MASQUERADE ---to-ports 1024-65535 
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -p udp -j MASQUERADE ---to-ports 1024-65535 
-A POSTROUTING -s 169.254.0.0/16 ! -d 169.254.0.0/16 -j MASQUERADE 
COMMIT
# Filtering and forwarding rules
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
# Catch-all, including NFS
-A INPUT -i virbr1 -p udp -j ACCEPT 
-A INPUT -i virbr1 -p tcp -j ACCEPT 
# Forward to/from private network
-A FORWARD -d 169.254.0.0/16 -o virbr1 -m state --state RELATED,ESTABLISHED --j ACCEPT 
-A FORWARD -s 169.254.0.0/16 -i virbr1 -j ACCEPT 
-A FORWARD -i virbr1 -o virbr1 -j ACCEPT 
-A FORWARD -o virbr1 -j REJECT --reject-with icmp-port-unreachable 
-A FORWARD -i virbr1 -j REJECT --reject-with icmp-port-unreachable 
COMMIT
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
# Create checksums for DHCP clients even when using virtio
-A POSTROUTING -o virbr1 -p udp -m udp --dport 68 -j CHECKSUM --checksum-fill 
COMMIT
</pre>

This approach allows you to integrate the extra rules needed for the VMs
private network into any existing rules you use at your site.

<p>
Vac performs a quick check of the current iptables rules at the start of each
cycle using the iptables-save command. If any obvious problems are identified, 
then &quot;Failed to match XXX in output of iptables-save.
Have the NAT rules been removed?&quot; lines will be included in
/var/log/vacd-factory . However, if you have an unusual set of rules, these 
checks may produce false warnings, and they are not an exhaustive check that
the rules you have are sufficient for Vac.

<h3>Logical volumes</h3>

<p>
Vac virtual machines can use logical volumes that exist on the factory machine
to provide additional, faster disk space. Normally these will be mounted at
/scratch by the VM, as they are on conventional grid worker nodes. 

<p>By
default, the block device associated with the logical volume is available to
the VM is vdb, but this can changed with the scratch_device option in a
[vmtype ...] section. 

<p>
For each virtual machine Vac will look for a logical volume
to use with it. The global volume_group option in [settings] (default
vac_volume_group) and the virtual machine's name are used to construct the
logical volume paths to try. For example, 
/dev/vac_volume_group/factory1-01.example.com

<p>
You must create the volume group and logical volumes
to be used by Vac, with something similar to this, where /dev/sda9 is a
physical volume (partition):
<pre>
vgcreate vac_volume_group /dev/sda9
lvcreate --name factory1-00.example.com -L 25G vac_volume_group
lvcreate --name factory1-01.example.com -L 25G vac_volume_group
lvcreate --name factory1-02.example.com -L 25G vac_volume_group
...
</pre>

<p>
During the creation of each virtual machine instance, Vac will identify the 
logical volume it has been told to use, measure its size using lvs, and 
provide it to the VM as vdb by default.
 
<h3>Installation: tar vs RPM</h3>

<p>
RPM is the recommended installation procedure, and RPMs are available
from the <a href="http://www.gridpp.ac.uk/vac/versions/">Downloads
directory</a> on the Vac website.

<p>
It is possible to install Vac from a tar file, using the install Makefile
target.

<h3>Configuration of the Vac space</h3>

<p>
The [settings] section must include a vac_space name, which is also used
as the virtual CE name. 

<p>
A separate [factories] section contains the single required option name
which has a space separated list of the fully qualified domain names of all
the factories in this Vac space, including this factory. The factories are
queried using UDP when a factory needs to decide which vmtype to start.
The Vac responder process on the factories replies to these queries with
a summary of the VM and the outcome of recent attempts to run a VM of each
vmtype.

<p>
For ease of management, the [factories] section can be placed in
a configuration file in /etc/vac.d which could be automatically generated 
and maintained from another source, such as the site's assets database.

<h3>Setting up vmtypes</h3>

<p>
One [vmtype ...] section must exist for each vmtype in the system, with
the name of the vmtype given in the section name, such as [vmtype example].
A vmtype name must only consist of lowercase letters, numbers, periods,
underscores, and hyphens. The vac.conf(5) man page lists the options
that can be give for each vmtype.

<p>
The [targetshares] section contains a list of vmtype=share pairs giving
the desired share of the total VMs available in this space for each
vmtype. The shares do not need to add up to 1.0, and if a share is not given
for a vmtype, then it is set to 0. The creation of new VMs can be completely
disabled by setting all shares to 0. Vac factories consult these shares
when deciding which vmtype to start as VMs become available.

<p>
For ease of management, the [targetshares] section can be placed in
a file in /etc/vac.d, which is convenient if
it is generated automatically or frequently edited by hand and pushed
out to the factory machines.

<p>
The experiment or VO responsible for each vmtype should supply 
step by step intructions on how to set up the rest of the [vmtype ...]
section and how to configure the files to be placed in its subdirectory
of /var/lib/vac/vmtypes .

<h2>Starting and stopping vacd</h2>

<p>
The Vac daemon, vacd, is started and stopped by /etc/rc.d/init.d/vacd 
on conjunction with the usual service and chkconfig commands. As the 
configuration files are reread at the start of each cycle (by default, 
one per minute) it is not necessary to restart vacd after changing the 
configuration.

<p>
Furthermore, as vacd rereads the current state of the VMs from status
files and the hypervisor at the start of each cycle, vacd can be 
restarted without disrupting running VMs or losing information about
their state. 
In most cases it will even be possible to upgrade vacd from one patch
level to another within the same minor release without having to
drain the factory of running VMs.

<h2>Using vac command</h2>

<p>
The vac(1) man page explains how the vac command can be used to
scan the current Vac space and display the VMs running, along with
statistics about their CPU load and wall clock time.

<h2>Setting up Nagios</h2>

<p>
The check-vacd script installed in /var/lib/vac/bin can be used with
Nagios to monitor the state of the vacd on a factory node. 

<p>
It can be run from the local Nagios nrpe daemon with a line like this
in its configuration file:

<pre>
command[check-vacd]=/var/lib/vac/bin/check-vacd 600
</pre>

which raises an alarm if the vacd heartbeat wasn't updated in the
last 600 seconds.

<h2>APEL accounting</h2>

<p>
When Vac detects that a VM has finished, it writes one line with
accounting information to two daily log files in /var/log/vacd-accounting .
One log file is in PBS format, and one in BLAHP format. Vac uses the 
UUID of the VM as the local and grid job ID, and the vmtype name as the
local user and group. A unique user DN is constructed from the components 
of the Vac space name. For example, vac01.example.com becomes
/DC=com/DC=example/DC=vac01 . If the accounting_fqan option is present in a
[vmtype ...]
section, then for VMs of that type the value of that option is included in
the log files as the user FQAN, which indicates the VO associated with the
VM.

<p>
The format of these accounting log files is designed to work with the
standard APEL PBS log file parser, which can be run on each factory machine
from cron to publish accounting records into the site's APEL publisher
database.

<p>
For APEL 2, a configuration file modelled on this example can be placed in 
/etc/glite-apel-pbs/parser-config-vac.xml:

<pre>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;ApelConfiguration enableDebugLogging="yes"&gt;
&lt;SiteName&gt;EXAMPLE-COM&lt;/SiteName&gt;
&lt;DBURL&gt;jdbc:mysql://apel.example.com:3306/accounting&lt;/DBURL&gt;
  &lt;DBUsername&gt;accounting&lt;/DBUsername&gt;
  &lt;DBPassword&gt;REPLACE-WITH-PASSWORD&lt;/DBPassword&gt;
  &lt;DBProcessor inspectTables="no"/&gt;

  &lt;EventLogProcessor&gt;
    &lt;Logs searchSubDirs="yes"
     reprocess="no"&gt;&lt;Dir&gt;/var/log/vacd-accounting&lt;/Dir&gt;&lt;/Logs&gt;
    &lt;Timezone&gt;UTC&lt;/Timezone&gt;
    &lt;SubmitHost&gt;vac01.example.com&lt;/SubmitHost&gt;
  &lt;/EventLogProcessor&gt;

  &lt;BlahdLogProcessor&gt;
    &lt;BlahdLogPrefix&gt;blahp.log-&lt;/BlahdLogPrefix&gt;
    &lt;Logs searchSubDirs="yes" reprocess="no"&gt;
      &lt;Dir&gt;/var/log/vacd-accounting&lt;/Dir&gt;
    &lt;/Logs&gt;
  &lt;/BlahdLogProcessor&gt;
&lt;/ApelConfiguration&gt;
</pre>

<p>
The SiteName is the EGI/WLCG site name used by the GOCDB, and 
the other example.com values should be replaced with your chosen names as 
before.

<p>
The parser can be run once a day by placing the file
glite-apel-pbs-parser-cron in /etc/cron.d:
<pre>
25 01 * * * root env APEL_HOME=/ /usr/bin/apel-pbs-log-parser  -f /etc/glite-apel-pbs/parser-config-vac.xml >> /var/log/apel.log 2>&amp;1
</pre>

<p>
A similar approach can be taken for APEL 3 based sites, using the new
APEL configuration file format and scripts.

<p>
In either case, it is necessary to ensure that HEPSPEC06 benchmark values are 
present in the publisher database, either by placing them in the BDII and 
using the APEL SpecUpdater, or by inserting them by hand in the 
database. New rows only need to be inserted if the benchmark changes for some
reason. 

<!--
<h2>Tuning backoff</h2>

<h2>Troubleshooting</h2>
-->

</body>
</html>
