<html>
<head>
<title>Vac site admin guide</title>
</head>
<body>

<h1 align=center>Vac Site Admin Guide<!-- version --></h1>

<p align=center><b>Andrew McNab &lt;Andrew.McNab&nbsp;AT&nbsp;cern.ch&gt;</b>

<h2>Quick start</h2>

<p>
By following this quick start recipe you can verify that your installation 
will work with Vac and see it creating and destroying virtual machines. You
will almost certainly want to start again from scratch by following the
step-by-step part of the Admin Guide so don't invest a lot of time here.
If you're already familiar with VMs, you could just skip straight there
but it's safest to go through the quick start to make sure the requirements
are all there.

<p>
To follow the quick start, you need an x86_64 Intel or AMD machine 
with hardware virtualization (Intel VT-x or AMD-V) enabled in its BIOS; and
the machine needs to be installed with a version of Scientific Linux 6, 
with libvirt installed and enabled. (In particular, the packages
libvirt, libvirt-client, libvirt-python, qemu-kvm, qemu-kvm-tools, and then
run &quot;service libvirtd restart&quot; to make sure libvirtd daemon is
running.)

<p>
Install the vac RPM, and download a recent compressed kvm-style CernVM batch 
image from the <a href="http://cernvm.cern.ch/portal/downloads">CernVM 
downloads page</a> to /tmp. Use zcat to extract the image itself to a
file in /tmp. It will be a large file: more than 9GB.

<p>
Copy the file /var/lib/vac/doc/testkvm.xml to /tmp as well, and edit the
&quot;source file=&quot; path and filename to point to your CernVM image.

<p>
At the command line, excecute
<pre>
virsh create --console testkvm.xml
</pre>
You should see the familiar Linux boot messages and eventually a login
prompt as the virtual machine boots using the CernVM image file as its root
filesystem. If all this doesn't happen, then something is wrong with your
installation or hardward virtualization isn't enabled. Please check the
libvirt documentation to try to identify where the problem is.

<p>
To get out of the login prompt, just press Ctrl and ] and then use the
command 
<pre>
virsh destroy testkvm
</pre>
to kill the VM. We're now ready to set up Vac itself.

<p>
To create another pristine disk image, use zcat again to make another copy
in /tmp and then use cp to make a sparse copy of this image in Vac's
area (you may need to change the exact version numbers in these file names):
<pre>
cd /tmp
zcat cernvm-batch-node-2.6.0-4-1-x86_64.hdd.gz >cernvm-batch-node-2.6.0-4-1-x86_64.hdd
cp --sparse=always cernvm-batch-node-2.6.0-4-1-x86_64.hdd /var/lib/vac/images
</pre>
We do this because zcat (and gunzip) won't create sparse files, but cp can
and they are much quicker to read and copy around.

<p>
Vac uses NFS to share some directories from the factory machine to its virtual
machines, and needs to have the standard NFS server installed and running.
It's not necessary to configure the NFS server, as Vac uses exportfs
commands to create and destroy exports dynamically. If you have any iptables
rules blocking NFS you must disable them before starting vacd to avoid 
needing to reset iptables after libvirtd has made its additions.

<p>
Now you need to create the /etc/vac.conf configuration file. Copy
/var/lib/vac/doc/example.vac.conf to /etc/vac.conf and read through its
comments. There are 6 lines you need to check and probably change.

<dl>
<dt><b>vac_space =</b> in [settings]
<dd>Set this to vac01 in your site's domain. So if your site is .example.com
then set it to vac01.example.com . A Vac space is a group of factory
machines that communicate with each other, and is equivalent to a subcluster
or subsite. A space's name is a fully qualified domain name (FQDN), and can be 
used as a virtual CE name where necessary in other systems.

<dt><b>names =</b> in [factories]
<dd>Since we're creating a space that contains a single factory machine, 
    set this to be the FQDN of the factory machine you're workng on.

<dt><b>total_machines</b>
<dd>Set this to the number of VMs to create and manage on this factory.
    Vac will create hostnames for the VMs from the factory name. For
    example, factory1.example.com will lead to factory1-00.example.com,
    factory1-01.example.com, ...
    
<dt><b>root_image =</b> in [vmtype example]
<dd>The path and filename given in this setting must point to the CernVM
    image you created for Vac in /var/lib/vac/images. Double check the
    version numbers are correct.
  
<dt><b>rootpublickey =</b> in [vmtype example]
<dd>This setting is not strictly necessary but is very useful. By copying
    an RSA key pair to /root/.ssh on the factory machine, or creating
    one with ssh-keygen you will be able to ssh into the VM as root and
    see how it is laid out and how it is running. If you don't
    place a public key at the location given in this setting, you need 
    to comment out this setting.
</dl>

<p>
The files needed for the example vmtype are installed by the RPM in
/var/lib/vac/vmtypes/example and with /etc/vac.conf done and the CernVM
image in place you're ready to go. Just do <b>service vacd restart</b>
to make sure vacd is running and look in the log files.

<p>
When vacd starts it forks a factory process that watches the VMs and
creates or destroys them as necessary; and a responder process that
replies to queries from factories about what is running on this host.
These two processes have separate log files as /var/log/vacd-factory
and /var/log/vacd-responder . 

<p>
In its log file, you should be able to see the factory
daemon trying to decide what to do and then creating the example
VM which runs for 10 minutes then shuts itself down. When deciding
what to do, the factory queries its own responder via UDP and this
should be visible in the responder's log file.

<p>
You should also be able to see the state of the VM using the
command <b>vac scan</b>, where vac is a command line tool that the
RPM installs in /usr/sbin.

<h2>Configuration step-by-step</h2>

<p>
This part of the guide covers the same groud as the quick start
guide but in a lot more detail. It's intended to help you choose
how best to configure your site.

<p>
The configuration file /etc/vac.conf use the Python ConfigParser syntax, 
which is similar to MS
Windows INI files. The file is divided into sections, with each section
name in square brackets. For example: [settings]. Each section contains
a series of option=value pairs.

For ease of management, three more optional files are read if they exist:
<ul>
<li>/etc/vac-virtualmachines.conf
<li>/etc/vac-factories.conf
<li>/etc/vac-targetshares.conf
</ul>

<p>
These files are named after important sections, but sections can be placed
in any of the four files, or all placed in /etc/vac.conf

<h3>Xen vs kvm</h3>

<p>
We recommend that hardware virtualization (eg Intel VT-x features)
with kvm is used for production. Vac also supports Xen 
paravirtualization which can run on old machines without hardware
virtualization but this is not supported on RHEL6/SL6. (6.x RPMs
are <a href="http://xen.crc.id.au/support/guides/install/">available
from Steven Haigh</a>.)

<h3>CernVM images</h3>

<p>
Vac currently requires the use of CernVM images with HEPiX 
contexualization based on EC2/ISO (&quot;CD-ROM&quot;) images.

<p>
You can download a recent compressed kvm-style CernVM batch 
image from the <a href="http://cernvm.cern.ch/portal/downloads">CernVM 
downloads page</a>. You can use gunzip or zcat to extract the image
itself. It will be a large file: more than 9GB. gunzip creates
non-sparse files but you can convert the image to sparse with cp:
<pre>
cd /tmp
zcat cernvm-batch-node-2.6.0-4-1-x86_64.hdd.gz >cernvm-batch-node-2.6.0-4-1-x86_64.hdd
cp --sparse=always cernvm-batch-node-2.6.0-4-1-x86_64.hdd /var/lib/vac/images
</pre>
Sparse files not only use less disk space and are quicker to 
copy, but they are also quicker to read.

<!--
<p>
The cernvm-batch-node-2.6.0-4-1-x86_64 image has been distributed with a
filesystem check interval of 6 months, and after 3rd April 2013 a check
automatically happens when it is used as a Linux root partition. You
can disable this with the following procedure:
<pre>
losetup /dev/loop7 cernvm-batch-node-2.6.0-4-1-x86_64.hdd
tune2fs -i 0 -c -1 /dev/loop7
losetup -d /dev/loop7
</pre>
(If loop7 is in use on your system, replace it with a different number.)
-->

<h3>DNS, IP, MAC</h3>

<p>
There are three networking scenarios supported:

<p> 
 1) Bridging in which you make a [virtualmachine x.y.z] section for
 each VM you want on a factory, giving the FQDN (x.y.z), and the MAC address
 with the mac= option. You can then allocate everything however you want in
 your DHCP and DNS servers, and have a free choice of FQDN, but you must 
 supply an IP address for each VM as part of that.
 
<p>
 2) Bridging, but with no [virtualmachine x.y.z] sections and total_machines 
 specified instead. Vac then creates the VM FQDNs from the factory name
 by adding -00, -01, ... So factory1.example.com has factory1-00.example.com,
 factory1-01.example.com, ... as its VMs. Vac looks up the IP addresses of
 these names in your DNS and creates unique MACs from them. This is
 a way of avoiding making [virtualmachine x.y.z] sections, as they are
 factory
 specific and means you can use the same configuration on all your
 factories.
 However, you still need to set up your DHCP and DNS as in (1).

<p> 
 3) NAT, again using total_machines. This time Vac creates the FQDNs as
 in (2), but now also chooses private IPs (in 192.168.86.0 by default) and
 makes MACs for them. Using libvirt NAT machinery means this network is
 hidden from the rest of the LAN and only visible from the factory and its
 VMs. libvirt configures the dnsmasq server to run dedicated DNS and DHCP
 servers on this private network.

<h3>Logical volumes</h3>

<p>
Vac virtual machines can use logical volumes that exist on the factory machine
to provide additional, faster disk space. Normally these will be mounted at
/scratch by the VM, as they are on conventional grid worker nodes. 

<p>By
default, the block device associated with the logical volume is available to
the VM has hdb, but this can changed with the scratch_device option in a
[vmtype ...] section. 

<p>
If you are using explicit [virtualmachine ...] sections to define your
available virtual machines, then the scratch_volume option is used to 
give the full path of the logical volume on the factory machine. For
example, /dev/vac_volume_group/vm1.example.com_scratch 

<p>
If you are implicitly defining virtual machines using the total_machines 
option, then for each virtual machine Vac will look for a logical volume
to use with it. The global volume_group option in [settings] (default
vac_volume_group) and the virtual machine's name is used to construct the
logical volume paths to try. For example,
/dev/vac_volume_group/vm1.example.com

<p>
In either scenario, you must create the volume group and logical volumes
to be used by Vac, with something similar to this, where /dev/sda9 is a
physical volume (partition):
<pre>
vgcreate vac_volume_group /dev/sda9
lvcreate --name vm1.example.com -L 25G vac_volume_group
lvcreate --name vm2.example.com -L 25G vac_volume_group
lvcreate --name vm3.example.com -L 25G vac_volume_group
...
</pre>

<p>
During the creation of each virtual machine instance, Vac will identify the 
logical volume it has been told to use, (re)format it, and provide it to the
VM as hdb by default.
 
<h3>Installation: tar vs RPM</h3>

<p>
RPM is the recommended installation procedure, and RPMs are available
from the <a href="http://www.gridpp.ac.uk/vac/versions/">Downloads
directory</a> on the Vac website.

<p>
It is possible to install Vac from a tar file, using the install Makefile
target.

<h3>Configuration of the Vac space</h3>

<p>
The [settings] section must include a vac_space name, which is also used
as the virtual CE name. 

<p>
A separate [factories] section contains the single required option name
which has a space separated list of the fully qualified domain names of all
the factories in this Vac space, including this factory. The factories are
queried using UDP when a factory needs to decide which vmtype to start.
The Vac responder process on the factories replies to these queries with
a summary of the VM and the outcome of recent attempts to run a VM of each
vmtype.

<p>
For ease of management, the [factories] section can be placed in
/etc/vac-factories.conf which could be automatically generated and
maintained from another source, such as the site's assets database.

<h3>Configuration of this factory and its virtualmachines</h3>

If the total_machines option is not given in [settings], then 
one [virtualmachine ...] section must exist for each virtual machine 
assigned to this factory machine, with its fully qualified domain name 
given in the section name, such as [virtualmachine vm1.example.com].
Each of these sections contain one or two option=value pairs that are 
specific to that virtualmachine.

<p>
For ease of management, the [virtualmachine ...] sections can be
placed in a seperate /etc/vac-virtualmachines.conf file. This is
particularly convenient as it can be used to concentrate all the
factory-specific information in one file so the same /etc/vac.conf
can be used on all factories.

<h3>Setting up vmtypes</h3>

<p>
One [vmtype ...] section must exist for each vmtype in the system, with
the name of the vmtype given in the section name, such as [vmtype example].
A vmtype name must only consist of lowercase letters, numbers, periods,
underscores, and hyphens. The vac.conf(5) man page lists the options
that can be give for each vmtype.

<p>
The [targetshares] section contains a list of vmtype=share pairs giving
the desired share of the total VMs available in this space for each
vmtype. The shares do not need to add up to 1.0, and if a share is not given
for a vmtype, then it is set to 0. The creation of new VMs can be completely
disabled by setting all shares to 0. Vac factories consult these shares
when deciding which vmtype to start as VMs become available.

<p>
For ease of management, the [targetshares] section can be placed in
a separate file, /etc/vac-targetshares.conf, which is convenient if
it is generated automatically or frequently edited by hand and pushed
out to the factory machines.

<h2>Using vac command</h2>

<p>
The vac(1) man page explains how the vac command can be used to
scan the current Vac space and display the VMs running, along with
statistics about their CPU load and wall clock time.

<h2>Setting up Nagios</h2>

<p>
The check-vacd script installed in /var/lib/vac/bin can be used with
Nagios to monitor the state of the vacd on a factory node. 

<p>
It can be run from the local Nagios nrpe daemon with a line like this
in its configuration file:

<pre>
command[check-vacd]=/var/lib/vac/bin/check-vacd 600
</pre>

which raises an alarm if the vacd heartbeat wasn't updated in the
last 600 seconds.

<!--
<h2>Setting up APEL</h2>

<h2>Tuning backoff</h2>

<h2>Troubleshooting</h2>
-->

</body>
</html>
